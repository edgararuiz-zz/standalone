---
title: "Spark Standalone/S3 & sparklyr"
output:
  html_document: default
  html_notebook: default
---

## Load Libraries

```{r, warning = FALSE, message = FALSE, eval = FALSE}
# Install Dev version of sparklyr from GitHub
devtools::install_github("rstudio/sparklyr", force = TRUE)
```

```{r}
# Load libraries
suppressMessages({
  library(sparklyr)
  library(tidyverse)
  library(DBI)
}) 
```

## Load AWS Credentials

```{r, eval = FALSE}
Sys.setenv(AWS_ACCESS_KEY_ID="[Your access key]")
Sys.setenv(AWS_SECRET_ACCESS_KEY="[Your secret access key]")
```
```{r}
# Load AWS credentials
source("localonly/aws-credentials.R")
```

## Connect to Spark

### Spark connection configuration


```{r}
if(file.exists("metastore_db/"))unlink("metastore_db", recursive = TRUE, force = TRUE)
```

```{r}
conf <- spark_config()
```

Tested all of these settings:

- spark.driver.cores
- spark.driver.memory
- spark.executor.cores
- **spark.executor.memory**
- spark.master.cores
- spark.master.memory
- **spark.memory.fraction**
- spark.worker.cores
- spark.worker.memory
- sparklyr.cores.local
- sparklyr.shell.executor-cores
- sparklyr.shell.executor-memory


```{r}
conf$spark.memory.fraction <- 0.9
conf$spark.executor.memory <- "14g"
```

### Default packages

Tried all the combinations these packages:

- com.amazonaws:aws-java-sdk-pom
- com.amazonaws:aws-java-sdk
- com.amazonaws:aws-java-sdk-s3
- org.apache.hadoop:hadoop-aws

Findings:

- **org.apache.hadoop:hadoop-aws:2.7.3** is the only required package in order to read a CSV file in s3a
- Adding **com.amazonaws:aws-java-sdk** breaks CSV read
- Adding **com.amazonaws:aws-java-sdk-pom** and **com.amazonaws:aws-java-sdk-s3** does not add or remove any performance or capability
- **com.amazonaws:aws-java-sdk-pom** by itself does not work


```{r}
conf$sparklyr.defaultPackages <- "org.apache.hadoop:hadoop-aws:2.7.3"
```

```{r}
sc <- spark_connect(master = "spark://ip-172-30-1-5.us-west-2.compute.internal:7077", 
                    spark_home = "/home/ubuntu/spark-2.1.0-bin-hadoop2.7/",
                    config =  conf)
```

```{r}
sql <- "
CREATE EXTERNAL TABLE hive_flights (
  Year INT, 
  Month INT, 
  DayofMonth INT, 
  DayOfWeek INT, 
  DepTime INT, 
  CRSDepTime INT, 
  ArrTime INT, 
  CRSArrTime INT, 
  UniqueCarrier STRING, 
  FlightNum INT, 
  TailNum INT, 
  ActualElapsedTime  INT,  
  CRSElapsedTime INT, 
  AirTime INT, 
  ArrDelay  INT,  
  DepDelay INT, 
  Origin STRING, 
  Dest STRING, 
  Distance INT, 
  TaxiIn  STRING, 
  TaxiOut STRING, 
  Cancelled INT, 
  CancellationCode INT, 
  Diverted INT, 
  CarrierDelay  INT, 
  WeatherDelay INT, 
  NASDelay INT, 
  SecurityDelay INT, 
  LateAircraftDelay INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' 
LOCATION 's3a://flights-data/'"
```

```{r}
DBI::dbGetQuery(sc, sql)
```

## Tidy Data & Model

```{r}
tidy_flights <- tbl(sc, "hive_flights") %>%
  filter(!is.na(Cancelled)) %>%
  mutate(canceled = ifelse(Cancelled == 1, "y","n")) %>%
  select(Origin, Dest, canceled, Cancelled) %>%
  sdf_register("spark_flights")

tbl_cache(sc, "spark_flights")
```

```{r}

sample_flights <- tidy_flights %>%
  sample_frac(1) %>%
  select(canceled, Dest) %>%
  sdf_register("sample_flights")

tbl_cache(sc, "sample_flights")
```


```{r}

rf <- ml_random_forest(sample_flights, response = "Cancelled", features = c("Dest"), type = c("classification"))

tf <- sdf_predict(rf, sample_flights)

tf %>% group_by(Cancelled, prediction) %>% tally

sample_spark <- spark_dataframe(sample_flights)

vector_assembler <- invoke_new_simple_transformer(sc, "org.apache.spark.ml.feature.VectorAssembler",
                                        list(
                                          setInputCols = list("canceled"),
                                          setOutputCol = "Dest"))



ti <- invoke_static(sc, "org.apache.spark.ml.classification.NaiveBayes", "fit", sample_spark )

```



```{r}

enumerate <- function(object, f, ...) {
  nm <- names(object)
  result <- lapply(seq_along(object), function(i) {
    f(nm[[i]], object[[i]], ...)
  })
  names(result) <- names(object)
  result
}



invoke_new_simple_transformer <- function(sc, class, arguments) {
  #sdf <- spark_dataframe(x)
  #sc <- spark_connection(sdf)

  # generate transformer
  transformer <- invoke_new(sc, class)

  # apply arguments
  enumerate(arguments, function(key, val) {
    if (is.function(val))
      transformer <<- val(transformer, sdf)
    else if (!identical(val, NULL))
      transformer <<- invoke(transformer, key, val)
  })

  # invoke transformer
  #transformed <- invoke(transformer, "transform", sdf)

  return(transformer)
  # register result
  #sdf_register(transformed)
}

```

