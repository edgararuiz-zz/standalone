---
title: "Spark Standalone/S3 & sparklyr"
output: html_notebook
---

## Load Libraries

```{r, warning = FALSE, message = FALSE, eval = FALSE}
# Install Dev version of sparklyr from GitHub
devtools::install_github("rstudio/sparklyr", force = TRUE)
```

```{r}
# Load libraries
library(sparklyr)
library(tidyverse)
```

## Load AWS Credentials

```{r, eval = FALSE}
Sys.setenv(AWS_ACCESS_KEY_ID="[Your access key]")
Sys.setenv(AWS_SECRET_ACCESS_KEY="[Your secret access key]")
```
```{r}
# Load AWS credentials
source("localonly/aws-credentials.R")
```

## Connect to Spark

### Spark connection configuration

```{r}
if(file.exists("metastore_db/"))unlink("metastore_db", recursive = TRUE, force = TRUE)


conf <- spark_config()
```

Tested all of these settings:

- spark.driver.cores
- spark.driver.memory
- spark.executor.cores
- **spark.executor.memory**
- spark.master.cores
- spark.master.memory
- **spark.memory.fraction**
- spark.worker.cores
- spark.worker.memory
- sparklyr.cores.local
- sparklyr.shell.executor-cores
- sparklyr.shell.executor-memory


```{r}
conf$spark.memory.fraction <- 0.9
conf$spark.executor.memory <- "14g"
```

### Default packages

Tried all the combinations these packages:

- com.amazonaws:aws-java-sdk-pom
- com.amazonaws:aws-java-sdk
- com.amazonaws:aws-java-sdk-s3
- org.apache.hadoop:hadoop-aws

Findings:

- **org.apache.hadoop:hadoop-aws:2.7.3** is the only required package in order to read a CSV file in s3a
- Adding **com.amazonaws:aws-java-sdk** breaks CSV read
- Adding **com.amazonaws:aws-java-sdk-pom** and **com.amazonaws:aws-java-sdk-s3** does not add or remove any performance or capability
- **com.amazonaws:aws-java-sdk-pom** by itself does not work


```{r}
conf$sparklyr.defaultPackages <- "org.apache.hadoop:hadoop-aws:2.7.3"
```

```{r}
sc <- spark_connect(master = "spark://ip-172-30-1-5.us-west-2.compute.internal:7077", 
                    spark_home = "/home/ubuntu/spark-2.1.0-bin-hadoop2.7/",
                    config =  conf)
```

## Load Data

```{r}
flights <- spark_read_csv(sc, name = "spark_flights",
                         path = "s3a://flights-data/*",
                         header = TRUE)
```

## Tidy Data & Model

```{r}
tidy_flights <- flights %>%
  mutate(arrdelay1 = as.numeric(ArrDelay),
    depdelay1 = as.numeric(DepDelay),
    distance1 = as.numeric(Distance)) %>%
  filter(!is.na(arrdelay1)) %>%
  select(arrdelay = arrdelay1, 
         depdelay = depdelay1,
         distance = distance1) %>%
  sdf_register("tidy_flights")
```

```{r}
sample_flights <- tidy_flights %>%
  sample_frac(1) %>%
  sdf_register("sample_flights")
```

```{r}
reg_model <- sample_flights %>%
  ml_linear_regression("arrdelay", c("depdelay", "distance"))
summary(reg_model)
```

```{r}
reg_predict <- sdf_predict(reg_model, tidy_flights) %>%
  mutate(diff = arrdelay / prediction) 

reg_predict
```

```{r}
spark_disconnect(sc)
```
