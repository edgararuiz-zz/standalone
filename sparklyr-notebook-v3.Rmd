---
title: "Spark Standalone/S3 & sparklyr"
output:
  html_document: default
  html_notebook: default
---

## Load Libraries

```{r, warning = FALSE, message = FALSE, eval = FALSE}
# Install Dev version of sparklyr from GitHub
devtools::install_github("rstudio/sparklyr", force = TRUE)
```

```{r}
# Load libraries
library(sparklyr)
library(tidyverse)
library(DBI)
```

## Load AWS Credentials

```{r, eval = FALSE}
Sys.setenv(AWS_ACCESS_KEY_ID="[Your access key]")
Sys.setenv(AWS_SECRET_ACCESS_KEY="[Your secret access key]")
```
```{r}
# Load AWS credentials
source("localonly/aws-credentials.R")
```

## Connect to Spark

### Spark connection configuration

```{r}
if(file.exists("metastore_db/"))unlink("metastore_db", recursive = TRUE, force = TRUE)


conf <- spark_config()
```

Tested all of these settings:

- spark.driver.cores
- spark.driver.memory
- spark.executor.cores
- **spark.executor.memory**
- spark.master.cores
- spark.master.memory
- **spark.memory.fraction**
- spark.worker.cores
- spark.worker.memory
- sparklyr.cores.local
- sparklyr.shell.executor-cores
- sparklyr.shell.executor-memory


```{r}
conf$spark.memory.fraction <- 0.9
conf$spark.executor.memory <- "14g"
```

### Default packages

Tried all the combinations these packages:

- com.amazonaws:aws-java-sdk-pom
- com.amazonaws:aws-java-sdk
- com.amazonaws:aws-java-sdk-s3
- org.apache.hadoop:hadoop-aws

Findings:

- **org.apache.hadoop:hadoop-aws:2.7.3** is the only required package in order to read a CSV file in s3a
- Adding **com.amazonaws:aws-java-sdk** breaks CSV read
- Adding **com.amazonaws:aws-java-sdk-pom** and **com.amazonaws:aws-java-sdk-s3** does not add or remove any performance or capability
- **com.amazonaws:aws-java-sdk-pom** by itself does not work


```{r}
conf$sparklyr.defaultPackages <- "org.apache.hadoop:hadoop-aws:2.7.3"
```

```{r}
sc <- spark_connect(master = "spark://ip-172-30-1-5.us-west-2.compute.internal:7077", 
                    spark_home = "/home/ubuntu/spark-2.1.0-bin-hadoop2.7/",
                    config =  conf)
```

```{r}


sql <- "
CREATE EXTERNAL TABLE hive_events (
  GLOBALEVENTID STRING,
  SQLDATE STRING,
  MonthYear	 STRING,
  Year STRING,
  FractionDate STRING,	
  Actor1Code STRING,
  Actor1Name STRING,	
  Actor1CountryCode STRING,
  Actor1KnownGroupCode STRING,
  Actor1EthnicCode STRING,
  Actor1Religion1Code STRING,
  Actor1Religion2Code STRING,
  Actor1Type1Code STRING,
  Actor1Type2Code STRING,
  Actor1Type3Code STRING,
  Actor2Code STRING,
  Actor2Name STRING,
  Actor2CountryCode STRING,
  Actor2KnownGroupCode STRING,
  Actor2EthnicCode STRING,
  Actor2Religion1Code STRING,
  Actor2Religion2Code STRING,
  Actor2Type1Code STRING,
  Actor2Type2Code STRING,
  Actor2Type3Code STRING,
  IsRootEvent STRING,
  EventCode STRING,
  EventBaseCode STRING,
  EventRootCode STRING,
  QuadClass STRING,
  GoldsteinScale STRING,
  NumMentions STRING,
  NumSources STRING,
  NumArticles STRING,
  AvgTone STRING,
  Actor1Geo_Type STRING,	
  Actor1Geo_FullName STRING,
  Actor1Geo_CountryCode STRING,
  Actor1Geo_ADM1Code STRING,
  Actor1Geo_Lat STRING,
  Actor1Geo_Long STRING,
  Actor1Geo_FeatureID STRING,
  Actor2Geo_Type STRING,
  Actor2Geo_FullName STRING,
  Actor2Geo_CountryCode STRING,
  Actor2Geo_ADM1Code STRING,
  Actor2Geo_Lat STRING,
  Actor2Geo_Long STRING,
  Actor2Geo_FeatureID STRING,
  ActionGeo_Type STRING,
  ActionGeo_FullName STRING,
  ActionGeo_CountryCode STRING,
  ActionGeo_ADM1Code STRING,
  ActionGeo_Lat STRING,
  ActionGeo_Long STRING,
  ActionGeo_FeatureID STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n' 
LOCATION 's3a://gdelt-open-data/events/'"


DBI::dbGetQuery(sc, sql)
```

## Tidy Data & Model

```{r}
daily_event <- tbl(sc, "hive_events") %>%
  select(Actor1Name, Actor2Name) %>%
  top_n(1000) %>%
  sdf_register("spark_one_day")

system.time(tbl_cache(sc, "spark_one_day"))
```
